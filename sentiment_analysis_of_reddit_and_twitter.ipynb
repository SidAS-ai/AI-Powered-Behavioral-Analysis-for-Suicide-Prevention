{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN20sIfPYakRdOHyhYlQfrQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidAS-ai/AI-Powered-Behavioral-Analysis-for-Suicide-Prevention/blob/main/sentiment_analysis_of_reddit_and_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy tweepy praw nltk vaderSentiment textblob scikit-learn folium plotly matplotlib seaborn spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TFA9-DBU6kPh",
        "outputId": "1f0f7ffa-43a2-493e-bf01-8792e65e706c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from folium) (2025.1.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: vaderSentiment, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Dy7CGblyAB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfIhv1u06kKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import tweepy\n",
        "import praw\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load spaCy model for named entity recognition (location extraction)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define crisis-related keywords for filtering\n",
        "CRISIS_KEYWORDS = [\n",
        "    \"depressed\", \"depression\", \"suicide\", \"suicidal\", \"kill myself\",\n",
        "    \"end my life\", \"addiction\", \"overdose\", \"anxious\", \"anxiety\",\n",
        "    \"overwhelmed\", \"hopeless\", \"helpless\", \"self harm\", \"cutting\"\n",
        "]\n",
        "\n",
        "# Define high-risk phrases that indicate immediate concern\n",
        "HIGH_RISK_PHRASES = [\n",
        "    \"want to die\", \"don't want to live\", \"kill myself\", \"end my life\",\n",
        "    \"no reason to live\", \"better off dead\", \"going to end it\",\n",
        "    \"suicide plan\", \"last day\", \"goodbye world\", \"final note\"\n",
        "]\n",
        "\n",
        "class SocialMediaDataExtractor:\n",
        "    def __init__(self, twitter_credentials=None, reddit_credentials=None):\n",
        "        self.twitter_api = self.setup_twitter_api(twitter_credentials) if twitter_credentials else None\n",
        "        self.reddit_api = self.setup_reddit_api(reddit_credentials) if reddit_credentials else None\n",
        "\n",
        "    def setup_twitter_api(self, credentials):\n",
        "        \"\"\"Set up Twitter/X API authentication\"\"\"\n",
        "        auth = tweepy.OAuthHandler(credentials['api_key'], credentials['api_secret'])\n",
        "        auth.set_access_token(credentials['access_token'], credentials['access_token_secret'])\n",
        "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "        return api\n",
        "\n",
        "    def setup_reddit_api(self, credentials):\n",
        "        \"\"\"Set up Reddit API authentication\"\"\"\n",
        "        reddit = praw.Reddit(\n",
        "            client_id=credentials['client_id'],\n",
        "            client_secret=credentials['client_secret'],\n",
        "            user_agent=credentials['user_agent']\n",
        "        )\n",
        "        return reddit\n",
        "\n",
        "    def extract_twitter_data(self, keywords, count=100):\n",
        "        \"\"\"Extract tweets based on keywords\"\"\"\n",
        "        all_tweets = []\n",
        "\n",
        "        for keyword in keywords:\n",
        "            try:\n",
        "                tweets = self.twitter_api.search_tweets(q=keyword, count=count, tweet_mode=\"extended\", lang=\"en\")\n",
        "                for tweet in tweets:\n",
        "                    tweet_data = {\n",
        "                        'id': tweet.id_str,\n",
        "                        'timestamp': tweet.created_at.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        'content': tweet.full_text,\n",
        "                        'likes': tweet.favorite_count,\n",
        "                        'retweets': tweet.retweet_count,\n",
        "                        'user_location': tweet.user.location if tweet.user.location else None,\n",
        "                        'coordinates': tweet.coordinates['coordinates'] if tweet.coordinates else None,\n",
        "                        'place': tweet.place.full_name if tweet.place else None,\n",
        "                        'source': 'Twitter'\n",
        "                    }\n",
        "                    all_tweets.append(tweet_data)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error extracting tweets for keyword '{keyword}': {e}\")\n",
        "\n",
        "        return all_tweets\n",
        "\n",
        "    def extract_reddit_data(self, keywords, limit=100):\n",
        "        \"\"\"Extract Reddit posts based on keywords\"\"\"\n",
        "        all_posts = []\n",
        "\n",
        "        subreddits = ['depression', 'SuicideWatch', 'addiction', 'mentalhealth', 'Anxiety']\n",
        "        for subreddit in subreddits:\n",
        "            try:\n",
        "                for submission in self.reddit_api.subreddit(subreddit).hot(limit=limit):\n",
        "                    # Check if any keyword is in the title or selftext\n",
        "                    if any(keyword.lower() in submission.title.lower() or\n",
        "                           (submission.selftext and keyword.lower() in submission.selftext.lower())\n",
        "                           for keyword in keywords):\n",
        "                        post_data = {\n",
        "                            'id': submission.id,\n",
        "                            'timestamp': datetime.datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            'title': submission.title,\n",
        "                            'content': submission.selftext,\n",
        "                            'score': submission.score,\n",
        "                            'comments': submission.num_comments,\n",
        "                            'subreddit': submission.subreddit.display_name,\n",
        "                            'source': 'Reddit'\n",
        "                        }\n",
        "                        all_posts.append(post_data)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error extracting posts from r/{subreddit}: {e}\")\n",
        "\n",
        "        return all_posts\n",
        "\n",
        "class TextPreprocessor:\n",
        "    @staticmethod\n",
        "    def preprocess_text(text):\n",
        "        \"\"\"Clean and preprocess text data\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove emojis\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        # Lemmatize\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Join tokens back into string\n",
        "        cleaned_text = ' '.join(tokens)\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "class DataProcessor:\n",
        "    @staticmethod\n",
        "    def create_dataset(twitter_data, reddit_data):\n",
        "        \"\"\"Combine and standardize data from different sources\"\"\"\n",
        "        combined_data = []\n",
        "\n",
        "        # Process Twitter data\n",
        "        for tweet in twitter_data:\n",
        "            item = {\n",
        "                'id': tweet['id'],\n",
        "                'timestamp': tweet['timestamp'],\n",
        "                'content': tweet['content'],\n",
        "                'cleaned_content': TextPreprocessor.preprocess_text(tweet['content']),\n",
        "                'engagement': tweet['likes'] + tweet['retweets'],\n",
        "                'location': tweet['user_location'],\n",
        "                'coordinates': tweet['coordinates'],\n",
        "                'place': tweet['place'],\n",
        "                'source': tweet['source']\n",
        "            }\n",
        "            combined_data.append(item)\n",
        "\n",
        "        # Process Reddit data\n",
        "        for post in reddit_data:\n",
        "            content = post['title'] + \" \" + (post['content'] if post['content'] else \"\")\n",
        "            item = {\n",
        "                'id': post['id'],\n",
        "                'timestamp': post['timestamp'],\n",
        "                'content': content,\n",
        "                'cleaned_content': TextPreprocessor.preprocess_text(content),\n",
        "                'engagement': post['score'] + post['comments'],\n",
        "                'location': None,  # Reddit doesn't provide location data\n",
        "                'coordinates': None,\n",
        "                'place': None,\n",
        "                'source': post['source'] + \" (r/\" + post['subreddit'] + \")\"\n",
        "            }\n",
        "            combined_data.append(item)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(combined_data)\n",
        "\n",
        "        return df\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    @staticmethod\n",
        "    def analyze_sentiment(df):\n",
        "        \"\"\"Apply sentiment analysis to the dataset\"\"\"\n",
        "        # Initialize sentiment analyzers\n",
        "        vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "        # Create empty columns for sentiment scores\n",
        "        df['vader_neg'] = 0.0\n",
        "        df['vader_neu'] = 0.0\n",
        "        df['vader_pos'] = 0.0\n",
        "        df['vader_compound'] = 0.0\n",
        "        df['textblob_polarity'] = 0.0\n",
        "        df['textblob_subjectivity'] = 0.0\n",
        "\n",
        "        # Apply sentiment analysis to each row\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row['content']\n",
        "\n",
        "            # VADER sentiment\n",
        "            vader_scores = vader.polarity_scores(text)\n",
        "            df.at[idx, 'vader_neg'] = vader_scores['neg']\n",
        "            df.at[idx, 'vader_neu'] = vader_scores['neu']\n",
        "            df.at[idx, 'vader_pos'] = vader_scores['pos']\n",
        "            df.at[idx, 'vader_compound'] = vader_scores['compound']\n",
        "\n",
        "            # TextBlob sentiment\n",
        "            blob = TextBlob(text)\n",
        "            df.at[idx, 'textblob_polarity'] = blob.sentiment.polarity\n",
        "            df.at[idx, 'textblob_subjectivity'] = blob.sentiment.subjectivity\n",
        "\n",
        "        # Determine sentiment category based on compound score\n",
        "        df['sentiment'] = df['vader_compound'].apply(\n",
        "            lambda x: 'Positive' if x >= 0.05 else ('Negative' if x <= -0.05 else 'Neutral')\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "class RiskClassifier:\n",
        "    @staticmethod\n",
        "    def classify_risk_level(df):\n",
        "        \"\"\"Classify posts based on risk level\"\"\"\n",
        "        # Function to check if any high-risk phrase is in the text\n",
        "        def contains_high_risk(text):\n",
        "            if not isinstance(text, str):\n",
        "                return False\n",
        "            text = text.lower()\n",
        "            return any(phrase in text for phrase in HIGH_RISK_PHRASES)\n",
        "\n",
        "        # Function to calculate TF-IDF similarity with crisis terms\n",
        "        def calculate_crisis_similarity(text):\n",
        "            if not isinstance(text, str) or text.strip() == \"\":\n",
        "                return 0\n",
        "\n",
        "            # Create a small corpus with crisis terms and the text\n",
        "            corpus = [\" \".join(CRISIS_KEYWORDS), text]\n",
        "\n",
        "            # Calculate TF-IDF\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "            return similarity\n",
        "\n",
        "        # Apply risk classification\n",
        "        df['contains_high_risk'] = df['content'].apply(contains_high_risk)\n",
        "        df['crisis_similarity'] = df['cleaned_content'].apply(calculate_crisis_similarity)\n",
        "\n",
        "        # Assign risk levels\n",
        "        def assign_risk_level(row):\n",
        "            if row['contains_high_risk']:\n",
        "                return 'High-Risk'\n",
        "            elif row['crisis_similarity'] > 0.3:\n",
        "                return 'Moderate Concern'\n",
        "            else:\n",
        "                return 'Low Concern'\n",
        "\n",
        "        df['risk_level'] = df.apply(assign_risk_level, axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "class Visualizer:\n",
        "    @staticmethod\n",
        "    def visualize_sentiment_risk(df):\n",
        "        \"\"\"Create visualizations for sentiment and risk categories\"\"\"\n",
        "        # Create a crosstab of sentiment and risk level\n",
        "        crosstab = pd.crosstab(df['sentiment'], df['risk_level'])\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot 1: Sentiment Distribution\n",
        "        plt.subplot(2, 2, 1)\n",
        "        sns.countplot(data=df, x='sentiment', palette='viridis')\n",
        "        plt.title('Sentiment Distribution')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Plot 2: Risk Level Distribution\n",
        "        plt.subplot(2, 2, 2)\n",
        "        sns.countplot(data=df, x='risk_level', palette='viridis')\n",
        "        plt.title('Risk Level Distribution')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Plot 3: Sentiment vs Risk Level\n",
        "        plt.subplot(2, 1, 2)\n",
        "        sns.heatmap(crosstab, annot=True, cmap='YlGnBu', fmt='d')\n",
        "        plt.title('Sentiment vs Risk Level')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sentiment_risk_analysis.png')\n",
        "        plt.close()\n",
        "\n",
        "        return crosstab\n",
        "\n",
        "class Geolocator:\n",
        "    @staticmethod\n",
        "    def extract_locations(df):\n",
        "        \"\"\"Extract location information from text and metadata\"\"\"\n",
        "        def extract_location_from_text(text):\n",
        "            if not isinstance(text, str):\n",
        "                return None\n",
        "\n",
        "            doc = nlp(text)\n",
        "            locations = []\n",
        "\n",
        "            # Extract locations from named entities\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in ['GPE', 'LOC']:\n",
        "                    locations.append(ent.text)\n",
        "\n",
        "            return locations[0] if locations else None\n",
        "\n",
        "        # Extract locations from text content\n",
        "        df['extracted_location'] = df['content'].apply(extract_location_from_text)\n",
        "\n",
        "        # Combine available location information\n",
        "        df['final_location'] = df['place'].fillna(df['location']).fillna(df['extracted_location'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def geocode_locations(df):\n",
        "        \"\"\"Convert location names to coordinates using a mock geocoding service\"\"\"\n",
        "        # This is a simplified mock geocoding function\n",
        "        # In a real-world scenario, you would use a geocoding service like Google Maps API\n",
        "        mock_geocoding = {\n",
        "            'new york': (40.7128, -74.0060),\n",
        "            'los angeles': (34.0522, -118.2437),\n",
        "            'chicago': (41.8781, -87.6298),\n",
        "            'houston': (29.7604, -95.3698),\n",
        "            'phoenix': (33.4484, -112.0740),\n",
        "            'philadelphia': (39.9526, -75.1652),\n",
        "            'san antonio': (29.4241, -98.4936),\n",
        "            'san diego': (32.7157, -117.1611),\n",
        "            'dallas': (32.7767, -96.7970),\n",
        "            'san jose': (37.3382, -121.8863),\n",
        "            'austin': (30.2672, -97.7431),\n",
        "            'jacksonville': (30.3322, -81.6557),\n",
        "            'san francisco': (37.7749, -122.4194),\n",
        "            'seattle': (47.6062, -122.3321),\n",
        "            'boston': (42.3601, -71.0589)\n",
        "        }\n",
        "\n",
        "        def get_coordinates(location):\n",
        "            if not isinstance(location, str):\n",
        "                return None\n",
        "\n",
        "            location = location.lower()\n",
        "\n",
        "            # Check if the location or part of it is in our mock database\n",
        "            for loc, coords in mock_geocoding.items():\n",
        "                if loc in location:\n",
        "                    return coords\n",
        "\n",
        "            return None\n",
        "\n",
        "        df['geocoded_coords'] = df['final_location'].apply(get_coordinates)\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def create_heatmap(df):\n",
        "        \"\"\"Create a heatmap of crisis-related posts\"\"\"\n",
        "        # Filter rows with valid coordinates\n",
        "        geo_df = df.dropna(subset=['geocoded_coords'])\n",
        "\n",
        "        if len(geo_df) == 0:\n",
        "            logging.warning(\"No valid coordinates found for mapping.\")\n",
        "            return None\n",
        "\n",
        "        # Extract coordinates\n",
        "        coordinates = geo_df['geocoded_coords'].tolist()\n",
        "\n",
        "        # Create a map centered on the US\n",
        "        m = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n",
        "\n",
        "        # Add heatmap layer\n",
        "        HeatMap(coordinates).add_to(m)\n",
        "\n",
        "        # Add markers for high-risk posts\n",
        "        high_risk_df = geo_df[geo_df['risk_level'] == 'High-Risk']\n",
        "        for _, row in high_risk_df.iterrows():\n",
        "            folium.Marker(\n",
        "                location=row['geocoded_coords'],\n",
        "                popup=f\"Risk Level: {row['risk_level']}<br>Sentiment: {row['sentiment']}\",\n",
        "                icon=folium.Icon(color='red', icon='info-sign')\n",
        "            ).add_to(m)\n",
        "\n",
        "        # Save the map\n",
        "        m.save('crisis_heatmap.html')\n",
        "\n",
        "        return m\n",
        "\n",
        "    @staticmethod\n",
        "    def top_crisis_locations(df):\n",
        "        \"\"\"Identify top locations with highest crisis discussions\"\"\"\n",
        "        # Count posts by location\n",
        "        location_counts = df.dropna(subset=['final_location'])['final_location'].value_counts().head(5)\n",
        "\n",
        "        # Create a bar chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        location_counts.plot(kind='bar', color='teal')\n",
        "        plt.title('Top 5 Locations with Highest Crisis Discussions')\n",
        "        plt.xlabel('Location')\n",
        "        plt.ylabel('Number of Posts')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('top_crisis_locations.png')\n",
        "        plt.close()\n",
        "\n",
        "        return location_counts\n",
        "\n",
        "class CrisisAnalysisPipeline:\n",
        "    def __init__(self, twitter_credentials=None, reddit_credentials=None, use_sample_data=True):\n",
        "        self.data_extractor = SocialMediaDataExtractor(twitter_credentials, reddit_credentials)\n",
        "        self.use_sample_data = use_sample_data\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Run the complete social media crisis analysis pipeline\"\"\"\n",
        "\n",
        "        # Step 1: Data Collection\n",
        "        logging.info(\"Step 1: Collecting social media data...\")\n",
        "\n",
        "        if self.use_sample_data:\n",
        "            # Use sample data for demonstration\n",
        "            logging.info(\"Using sample data for demonstration...\")\n",
        "\n",
        "            # Create sample Twitter data\n",
        "            sample_twitter_data = [\n",
        "                {\n",
        "                    'id': '1234567890',\n",
        "                    'timestamp': '2025-03-15 10:30:45',\n",
        "                    'content': 'I feel so depressed lately. Nothing seems to help. #mentalhealth',\n",
        "                    'likes': 5,\n",
        "                    'retweets': 2,\n",
        "                    'user_location': 'New York, NY',\n",
        "                    'coordinates': None,\n",
        "                    'place': 'New York',\n",
        "                    'source': 'Twitter'\n",
        "                },\n",
        "                {\n",
        "                    'id': '0987654321',\n",
        "                    'timestamp': '2025-03-15 09:15:30',\n",
        "                    'content': 'The anxiety is overwhelming. I don\\'t know how much longer I can take this.',\n",
        "                    'likes': 10,\n",
        "                    'retweets': 3,\n",
        "                    'user_location': 'Los Angeles, CA',\n",
        "                    'coordinates': None,\n",
        "                    'place': 'Los Angeles',\n",
        "                    'source': 'Twitter'\n",
        "                },\n",
        "                {\n",
        "                    'id': '5678901234',\n",
        "                    'timestamp': '2025-03-15 11:45:22',\n",
        "                    'content': 'I want to die. Nobody would even notice if I was gone. #suicidal',\n",
        "                    'likes': 3,\n",
        "                    'retweets': 1,\n",
        "                    'user_location': 'Chicago, IL',\n",
        "                    'coordinates': None,\n",
        "                    'place': 'Chicago',\n",
        "                    'source': 'Twitter'\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Create sample Reddit data\n",
        "            sample_reddit_data = [\n",
        "                {\n",
        "                    'id': 'abc123',\n",
        "                    'timestamp': '2025-03-15 14:20:15',\n",
        "                    'title': 'Need help with addiction',\n",
        "                    'content': 'I\\'ve been struggling with substance abuse for years and I don\\'t know where to turn.',\n",
        "                    'score': 25,\n",
        "                    'comments': 12,\n",
        "                    'subreddit': 'addiction',\n",
        "                    'source': 'Reddit'\n",
        "                },\n",
        "                {\n",
        "                    'id': 'def456',\n",
        "                    'timestamp': '2025-03-15 16:05:33',\n",
        "                    'title': 'Lost all hope',\n",
        "                    'content': 'I don\\'t see any reason to keep going. I\\'ve tried everything and nothing works.',\n",
        "                    'score': 15,\n",
        "                    'comments': 8,\n",
        "                    'subreddit': 'depression',\n",
        "                    'source': 'Reddit'\n",
        "                },\n",
        "                {\n",
        "                    'id': 'ghi789',\n",
        "                    'timestamp': '2025-03-15 17:30:10',\n",
        "                    'title': 'Overwhelmed in Boston',\n",
        "                    'content': 'Living in Boston and feeling completely overwhelmed with life. Can anyone in the area recommend resources?',\n",
        "                    'score': 20,\n",
        "                    'comments': 15,\n",
        "                    'subreddit': 'mentalhealth',\n",
        "                    'source': 'Reddit'\n",
        "                }\n",
        "            ]\n",
        "        else:\n",
        "            # Use real API connections\n",
        "            if self.data_extractor.twitter_api:\n",
        "                sample_twitter_data = self.data_extractor.extract_twitter_data(CRISIS_KEYWORDS)\n",
        "            else:\n",
        "                sample_twitter_data = []\n",
        "                logging.warning(\"No Twitter credentials provided. Skipping Twitter data collection.\")\n",
        "\n",
        "            if self.data_extractor.reddit_api:\n",
        "                sample_reddit_data = self.data_extractor.extract_reddit_data(CRISIS_KEYWORDS)\n",
        "            else:\n",
        "                sample_reddit_data = []\n",
        "                logging.warning(\"No Reddit credentials provided. Skipping Reddit data collection.\")\n",
        "\n",
        "        # Step 2: Create and preprocess dataset\n",
        "        logging.info(\"Step 2: Creating and preprocessing dataset...\")\n",
        "        df = DataProcessor.create_dataset(sample_twitter_data, sample_reddit_data)\n",
        "\n",
        "        # Save the clean dataset\n",
        "        df.to_csv('cleaned_crisis_data.csv', index=False)\n",
        "        logging.info(f\"Cleaned dataset saved with {len(df)} entries.\")\n",
        "\n",
        "        # Step 3: Sentiment Analysis\n",
        "        logging.info(\"Step 3: Applying sentiment analysis...\")\n",
        "        df = SentimentAnalyzer.analyze_sentiment(df)\n",
        "\n",
        "        # Step 4: Risk Classification\n",
        "        logging.info(\"Step 4: Classifying risk levels...\")\n",
        "        df = RiskClassifier.classify_risk_level(df)\n",
        "\n",
        "        # Step 5: Visualize sentiment and risk\n",
        "        logging.info(\"Step 5: Creating sentiment and risk visualizations...\")\n",
        "        sentiment_risk_table = Visualizer.visualize_sentiment_risk(df)\n",
        "        logging.info(\"Sentiment and risk visualizations created.\")\n",
        "\n",
        "        # Step 6: Extract and geocode locations\n",
        "        logging.info(\"Step 6: Extracting and geocoding locations...\")\n",
        "        df = Geolocator.extract_locations(df)\n",
        "        df = Geolocator.geocode_locations(df)\n",
        "\n",
        "        # Step 7: Create heatmap\n",
        "        logging.info(\"Step 7: Creating crisis heatmap...\")\n",
        "        heatmap = Geolocator.create_heatmap(df)\n",
        "        if heatmap:\n",
        "            logging.info(\"Crisis heatmap created and saved as 'crisis_heatmap.html'.\")\n",
        "\n",
        "        # Step 8: Identify top crisis locations\n",
        "        logging.info(\"Step 8: Identifying top crisis locations...\")\n",
        "        top_locations = Geolocator.top_crisis_locations(df)\n",
        "        logging.info(\"Top crisis locations identified and visualization saved.\")\n",
        "\n",
        "        # Return the processed dataframe\n",
        "        return df\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the pipeline with sample data\n",
        "    pipeline = CrisisAnalysisPipeline(use_sample_data=True)\n",
        "    crisis_data = pipeline.run_pipeline()\n",
        "    logging.info(\"Crisis analysis pipeline completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhuHQLjw6kM8",
        "outputId": "d60719fd-5e05-471a-bf91-ab39598d0bd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "<ipython-input-7-ec6b6efa1e61>:297: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(data=df, x='sentiment', palette='viridis')\n",
            "<ipython-input-7-ec6b6efa1e61>:303: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(data=df, x='risk_level', palette='viridis')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import"
      ],
      "metadata": {
        "id": "qFjAEZyQ6kSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}